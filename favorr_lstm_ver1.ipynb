{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Predicting if a user write at least a review from a session\n",
    "# use LSTM\n",
    "#\n",
    "\n",
    "# import modules\n",
    "from pymongo import MongoClient # use mongodb\n",
    "\n",
    "import numpy as np # use numpay\n",
    "from numpy import genfromtxt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read data from csv file\n",
    "# 24 actions for 1 session. Each actions have unique indexes.\n",
    "# like 'notifications-view' -> 23\n",
    "my_data_1 = genfromtxt('session_data.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# shuffle data\n",
    "np.random.shuffle(my_data_1)\n",
    "\n",
    "# trim data to match batch size * batch count because somehow tf show erros without this...\n",
    "my_data_2 = my_data_1[:7692]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1.,   2.,   3.,  78.,  78.,  78.,  78.,  78.,  78.,  78.,  78.,\n",
       "        78.,  78.,  78.,  78.,  78.,  78.,  78.,  78.,  78.,  78.,  78.,\n",
       "        78.,  78.,   0.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check data\n",
    "my_data_2[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# convert list to numpy array and divide into train and valid data\n",
    "# since we don't have many data, I don't prepare test set which we can test real future data\n",
    "x_list = np.array(my_data_2[:,:24])\n",
    "y_list = np.array(my_data_2[:,24])\n",
    "\n",
    "train_x = x_list[:7200]\n",
    "train_y = y_list[:7200]\n",
    "valid_x = x_list[:-7200]\n",
    "valid_y = y_list[:-7200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# helper function to get batch \n",
    "def get_batch(x, y, batch_idx, batch_size): \n",
    "    \n",
    "    rtn_x = x[batch_idx*batch_size:(batch_idx+1)*batch_size]\n",
    "    rtn_y = y[batch_idx*batch_size:(batch_idx+1)*batch_size]\n",
    "    rtn_y = rtn_y.reshape((rtn_y.shape[0],1))\n",
    "    \n",
    "    return rtn_x, rtn_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# set hyper parameters \n",
    "# since I refer multiple examples and convined them. \n",
    "\n",
    "# reset graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "# training_iters = 100\n",
    "training_iters = 360 # 360 was just good enough\n",
    "# batch_size = 128\n",
    "batch_size = 12\n",
    "display_step = 10\n",
    "\n",
    "num_actions = 78\n",
    "\n",
    "# Network Parameters\n",
    "num_input = 1\n",
    "num_steps = 24\n",
    "# n_hidden = 32 # hidden layer num of features\n",
    "lstm_size = 32\n",
    "\n",
    "n_classes = 2 # total classes (yes or no)\n",
    "num_classes = 2\n",
    "\n",
    "# I have no idea. I just copied example's setting\n",
    "num_layers = 2\n",
    "\n",
    "# tf Graph input\n",
    "\n",
    "# x\n",
    "inputs = tf.placeholder(tf.int32, [None, num_steps], name='inputs')\n",
    "\n",
    "# y\n",
    "targets = tf.placeholder(tf.int32, [None, 1], name='targets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define model\n",
    "\n",
    "# Keep probability placeholder for drop out layers\n",
    "keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "# One-hot encoding the input and target characters\n",
    "x_one_hot = tf.one_hot(inputs, num_actions)\n",
    "y_one_hot = tf.one_hot(targets, num_classes)\n",
    "\n",
    "# Use a basic LSTM cell\n",
    "lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "\n",
    "# Add dropout to the cell\n",
    "drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "\n",
    "# Stack up multiple LSTM layers, for deep learning\n",
    "cell = tf.contrib.rnn.MultiRNNCell([drop] * num_layers)\n",
    "initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "### Run the data through the RNN layers\n",
    "# This makes a list where each element is on step in the sequence\n",
    "rnn_inputs = [tf.squeeze(i, squeeze_dims=[1]) for i in tf.split(x_one_hot, num_steps, 1)]\n",
    "\n",
    "# Run each sequence step through the RNN and collect the outputs\n",
    "outputs, state = tf.contrib.rnn.static_rnn(cell, rnn_inputs, initial_state=initial_state)\n",
    "final_state = state\n",
    "output_last = outputs[-1]\n",
    "\n",
    "# Reshape output so it's a bunch of rows, one output row for each step for each batch\n",
    "# seq_output = tf.concat(outputs, axis=1)\n",
    "# output = tf.reshape(seq_output, [-1, lstm_size])\n",
    "\n",
    "# Now connect the RNN putputs to a softmax layer\n",
    "with tf.variable_scope('softmax'):\n",
    "    softmax_w = tf.Variable(tf.truncated_normal((lstm_size, num_classes), stddev=0.1))\n",
    "    softmax_b = tf.Variable(tf.zeros(num_classes))\n",
    "\n",
    "# Since output is a bunch of rows of RNN cell outputs, logits will be a bunch\n",
    "# of rows of logit outputs, one for each step and batch\n",
    "# logits = tf.matmul(output, softmax_w) + softmax_b\n",
    "# we care only the last\n",
    "\n",
    "# calculate logits from output_last and weight and bias\n",
    "logits = tf.matmul(output_last, softmax_w) + softmax_b\n",
    "\n",
    "# Use softmax to get the probabilities for predicted characters\n",
    "preds = tf.nn.softmax(logits, name='predictions')\n",
    "\n",
    "# Reshape the targets to match the logits\n",
    "y_reshaped = tf.reshape(y_one_hot, [-1, num_classes])\n",
    "loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_reshaped)\n",
    "cost = tf.reduce_mean(loss)\n",
    "\n",
    "# Optimizer for training, using gradient clipping to control exploding gradients\n",
    "# I don't know we still needs settings below...\n",
    "\n",
    "# unkonwn settings\n",
    "grad_clip = 5\n",
    "\n",
    "tvars = tf.trainable_variables()\n",
    "grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars), grad_clip)\n",
    "train_op = tf.train.AdamOptimizer(learning_rate)\n",
    "optimizer = train_op.apply_gradients(zip(grads, tvars))\n",
    "\n",
    "# Export the nodes\n",
    "# NOTE: I'm using a namedtuple here because I think they are cool\n",
    "export_nodes = ['inputs', 'targets', 'initial_state', 'final_state',\n",
    "                'keep_prob', 'cost', 'preds', 'optimizer']\n",
    "Graph = namedtuple('Graph', export_nodes)\n",
    "local_dict = locals()\n",
    "graph = Graph(*[local_dict[each] for each in export_nodes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# accuracy\n",
    "correct_pred = tf.equal(tf.argmax(preds,1), tf.argmax(tf.squeeze(y_one_hot),1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_x.shape:(12, 24)\n",
      "batch_y.shape:(12, 1)\n",
      "step:10, test loss:0.24739871314105888, valid accuracy:0.8983739876165623\n",
      "step:20, test loss:0.22733348609569173, valid accuracy:0.8882113840521836\n",
      "step:30, test loss:0.2087030749814585, valid accuracy:0.9166666705433916\n",
      "step:40, test loss:0.19588029901497067, valid accuracy:0.9268292712002266\n",
      "step:50, test loss:0.1778119153017178, valid accuracy:0.9329268336296082\n",
      "step:60, test loss:0.16481270728458186, valid accuracy:0.9308943123352237\n",
      "step:70, test loss:0.15459845179129236, valid accuracy:0.9390243960589897\n",
      "step:80, test loss:0.14687605613512764, valid accuracy:0.9451219570345994\n",
      "step:90, test loss:0.1356642262970369, valid accuracy:0.9512195180102092\n",
      "step:100, test loss:0.131806499362865, valid accuracy:0.9552845605989781\n",
      "step:110, test loss:0.12457033184250274, valid accuracy:0.9593496002802034\n",
      "step:120, test loss:0.11956621218453317, valid accuracy:0.9532520422121373\n",
      "step:130, test loss:0.11334160256189837, valid accuracy:0.9532520393045937\n",
      "step:140, test loss:0.1145522193776075, valid accuracy:0.9593496017339753\n",
      "step:150, test loss:0.10442639557329433, valid accuracy:0.9573170775320472\n",
      "step:160, test loss:0.10073960410877286, valid accuracy:0.9573170760782753\n",
      "step:170, test loss:0.09682553568893733, valid accuracy:0.9634146399614287\n",
      "step:180, test loss:0.0980884440708663, valid accuracy:0.9654471598020414\n",
      "step:190, test loss:0.09160355228101252, valid accuracy:0.971544722231423\n",
      "step:200, test loss:0.08909778274440518, valid accuracy:0.9634146399614287\n",
      "step:210, test loss:0.08669327054839716, valid accuracy:0.9674796810964259\n",
      "step:220, test loss:0.08908536336782466, valid accuracy:0.9654471598020414\n",
      "step:230, test loss:0.08721077859668488, valid accuracy:0.9695122009370385\n",
      "step:240, test loss:0.08532478278274842, valid accuracy:0.9654471612558132\n",
      "step:250, test loss:0.08161368799444366, valid accuracy:0.9613821201208161\n",
      "step:260, test loss:0.0825986283631452, valid accuracy:0.971544722231423\n",
      "step:270, test loss:0.08228847236243989, valid accuracy:0.967479679642654\n",
      "step:280, test loss:0.0865091389498654, valid accuracy:0.971544722231423\n",
      "step:290, test loss:0.07950105715336879, valid accuracy:0.9796748001401018\n",
      "step:300, test loss:0.07910409171401019, valid accuracy:0.971544722231423\n",
      "step:310, test loss:0.07912374068730665, valid accuracy:0.9654471627095851\n",
      "step:320, test loss:0.07849750690906755, valid accuracy:0.9735772406182638\n",
      "step:330, test loss:0.07734142038214487, valid accuracy:0.9695121994832667\n",
      "step:340, test loss:0.07727799111084702, valid accuracy:0.9735772420720357\n",
      "step:350, test loss:0.07493538479939162, valid accuracy:0.9715447207776512\n",
      "Optimization Finished!\n"
     ]
    }
   ],
   "source": [
    "# Launch the graph\n",
    "\n",
    "# Save check points\n",
    "save_file = './favorr_lstm_02.ckpt'\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    step = 1\n",
    "    # Keep training until reach max iterations\n",
    "    # while step * batch_size < training_iters:\n",
    "    # training_iters = len(train_x) // batch_size\n",
    "    # print(training_iters)\n",
    "    while step < training_iters:\n",
    "        # batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        \n",
    "        # total loss\n",
    "        loss_total = 0\n",
    "        \n",
    "        batch_count = len(train_x) // batch_size\n",
    "        for batch_idx in range(batch_count):\n",
    "            batch_x, batch_y = get_batch(train_x, train_y, batch_idx, batch_size)\n",
    "            sess.run(optimizer, feed_dict={inputs: batch_x, targets: batch_y, keep_prob:0.7})\n",
    "            \n",
    "            if batch_idx == 0 and step == 1:\n",
    "                print(\"batch_x.shape:{}\".format(batch_x.shape))\n",
    "                print(\"batch_y.shape:{}\".format(batch_y.shape))\n",
    "        \n",
    "            # calculate loss\n",
    "            loss = sess.run(cost, feed_dict={inputs: batch_x, targets: batch_y, keep_prob:1.0})\n",
    "            loss_total = loss_total + loss\n",
    "        \n",
    "        \n",
    "        if step % display_step == 0:\n",
    "            \n",
    "            test_loss = loss_total / batch_count\n",
    "            \n",
    "            # Calculate validation accuracy\n",
    "            valid_batch_count = len(valid_x) // batch_size\n",
    "            valid_acc_total = 0\n",
    "            for batch_idx in range(valid_batch_count):\n",
    "                vaild_batch_x, valid_batch_y = get_batch(valid_x, valid_y, batch_idx, batch_size)\n",
    "                acc = sess.run(accuracy, feed_dict={inputs: vaild_batch_x, targets: valid_batch_y, keep_prob:1.0})\n",
    "                valid_acc_total = valid_acc_total + acc\n",
    "            \n",
    "            valid_acc = valid_acc_total / valid_batch_count\n",
    "            print(\"step:{}, test loss:{}, valid accuracy:{}\".format(step, test_loss, valid_acc))\n",
    "            \n",
    "\n",
    "        step += 1\n",
    "    \n",
    "    #\n",
    "    # finally Save the model\n",
    "    saver.save(sess, save_file)\n",
    "    print (\"Optimization Finished!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
