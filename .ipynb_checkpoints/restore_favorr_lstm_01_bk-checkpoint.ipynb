{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Predicting Favorr Predicting One case:\n",
    "# version 6 use LSTM\n",
    "#\n",
    "\n",
    "# import modules\n",
    "from pymongo import MongoClient # use mongodb\n",
    "\n",
    "import numpy as np # use numpay\n",
    "from numpy import genfromtxt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "import time\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# restore checkpoint and make predition\n",
    "\n",
    "save_file = './favorr_lstm_01.ckpt'\n",
    "\n",
    "# test_x = np.array([[ 7, 3, 13, 14, 7, 3, 4, 7, 10, 8, 9, 21, 7, 10, 8, 9, 21, 7, 10, 8, 9, 21, 78, 78 ]])\n",
    "\n",
    "test_x = np.array([[ 73,\n",
    "  78,\n",
    "  78,\n",
    "  78,\n",
    "  78,\n",
    "  78,\n",
    "  78,\n",
    "  78,\n",
    "  78,\n",
    "  78,\n",
    "  78,\n",
    "  78,\n",
    "  78,\n",
    "  78,\n",
    "  78,\n",
    "  78,\n",
    "  78,\n",
    "  78,\n",
    "  78,\n",
    "  78,\n",
    "  78,\n",
    "  78,\n",
    "  78,\n",
    "  78 ]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reset graph and set parameters\n",
    "\n",
    "# reset graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "# training_iters = 100\n",
    "training_iters = 100\n",
    "# batch_size = 128\n",
    "batch_size = 12\n",
    "display_step = 10\n",
    "\n",
    "num_actions = 78\n",
    "\n",
    "# Network Parameters\n",
    "n_input = 1 # input (img shape: 28*28)\n",
    "num_input = 1\n",
    "# n_steps = 24 # timesteps\n",
    "num_steps = 24\n",
    "n_hidden = 32 # hidden layer num of features\n",
    "lstm_size = 32\n",
    "\n",
    "n_classes = 2 # total classes (yes or no)\n",
    "num_classes = 2\n",
    "\n",
    "num_layers = 2\n",
    "\n",
    "# tf Graph input\n",
    "# inputs = tf.placeholder(tf.int32, [batch_size, num_steps], name='inputs')\n",
    "# targets = tf.placeholder(tf.int32, [batch_size, 1], name='targets')\n",
    "inputs = tf.placeholder(tf.int32, [1, num_steps], name='inputs')\n",
    "targets = tf.placeholder(tf.int32, [1, 1], name='targets')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# re defing model\n",
    "\n",
    "# define model\n",
    "\n",
    "# Keep probability placeholder for drop out layers\n",
    "keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "# One-hot encoding the input and target characters\n",
    "x_one_hot = tf.one_hot(inputs, num_actions)\n",
    "y_one_hot = tf.one_hot(targets, num_classes)\n",
    "\n",
    "# Use a basic LSTM cell\n",
    "# lstm = tf.contrib.rnn.BasicLSTMCell(n_hidden)\n",
    "lstm = tf.contrib.rnn.BasicLSTMCell(n_hidden)\n",
    "\n",
    "# Add dropout to the cell\n",
    "drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "\n",
    "# Stack up multiple LSTM layers, for deep learning\n",
    "cell = tf.contrib.rnn.MultiRNNCell([drop] * num_layers)\n",
    "# initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "initial_state = cell.zero_state(1, tf.float32)\n",
    "\n",
    "### Run the data through the RNN layers\n",
    "# This makes a list where each element is on step in the sequence\n",
    "rnn_inputs = [tf.squeeze(i, squeeze_dims=[1]) for i in tf.split(x_one_hot, num_steps, 1)]\n",
    "\n",
    "# Run each sequence step through the RNN and collect the outputs\n",
    "outputs, state = tf.contrib.rnn.static_rnn(cell, rnn_inputs, initial_state=initial_state)\n",
    "final_state = state\n",
    "output_last = outputs[-1]\n",
    "\n",
    "# Reshape output so it's a bunch of rows, one output row for each step for each batch\n",
    "# seq_output = tf.concat(outputs, axis=1)\n",
    "# output = tf.reshape(seq_output, [-1, lstm_size])\n",
    "\n",
    "# Now connect the RNN putputs to a softmax layer\n",
    "with tf.variable_scope('softmax'):\n",
    "    softmax_w = tf.Variable(tf.truncated_normal((lstm_size, num_classes), stddev=0.1))\n",
    "    softmax_b = tf.Variable(tf.zeros(num_classes))\n",
    "\n",
    "# calculate logits\n",
    "logits = tf.matmul(output_last, softmax_w) + softmax_b\n",
    "\n",
    "# Use softmax to get the probabilities for predicted characters\n",
    "preds = tf.nn.softmax(logits, name='predictions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  6.24811393e-04   9.99375165e-01]]\n",
      "How!!!\n"
     ]
    }
   ],
   "source": [
    "# ran graph\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Test Cases\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    saver.restore(sess, save_file)\n",
    "    print(sess.run(preds, feed_dict={inputs: test_x, keep_prob:1.0}))\n",
    "\n",
    "print('How!!!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
