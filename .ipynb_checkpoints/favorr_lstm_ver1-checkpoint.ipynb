{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Predicting if a user write at least a review from a session\n",
    "# use LSTM\n",
    "#\n",
    "\n",
    "# import modules\n",
    "from pymongo import MongoClient # use mongodb\n",
    "\n",
    "import numpy as np # use numpay\n",
    "from numpy import genfromtxt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read data from csv file\n",
    "# 24 actions for 1 session. Each actions have unique indexes.\n",
    "# like 'notifications-view' -> 23\n",
    "my_data_1 = genfromtxt('session_data.csv', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# shuffle data\n",
    "np.random.shuffle(my_data_1)\n",
    "\n",
    "# trim data to match batch size * batch count because somehow tf show erros without this...\n",
    "my_data_2 = my_data_1[:7692]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1.,   2.,   3.,  78.,  78.,  78.,  78.,  78.,  78.,  78.,  78.,\n",
       "        78.,  78.,  78.,  78.,  78.,  78.,  78.,  78.,  78.,  78.,  78.,\n",
       "        78.,  78.,   0.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check data\n",
    "my_data_2[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# convert list to numpy array and divide into train and valid data\n",
    "# since we don't have many data, I don't prepare test set which we can test real future data\n",
    "x_list = np.array(my_data_2[:,:24])\n",
    "y_list = np.array(my_data_2[:,24])\n",
    "\n",
    "train_x = x_list[:7200]\n",
    "train_y = y_list[:7200]\n",
    "valid_x = x_list[:-7200]\n",
    "valid_y = y_list[:-7200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# helper function to get batch \n",
    "def get_batch(x, y, batch_idx, batch_size): \n",
    "    \n",
    "    rtn_x = x[batch_idx*batch_size:(batch_idx+1)*batch_size]\n",
    "    rtn_y = y[batch_idx*batch_size:(batch_idx+1)*batch_size]\n",
    "    rtn_y = rtn_y.reshape((rtn_y.shape[0],1))\n",
    "    \n",
    "    return rtn_x, rtn_y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# set hyper parameters \n",
    "# since I refer multiple examples and convined them. \n",
    "\n",
    "# reset graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "# training_iters = 100\n",
    "training_iters = 360 # 360 was just good enough\n",
    "# batch_size = 128\n",
    "batch_size = 12\n",
    "display_step = 10\n",
    "\n",
    "num_actions = 78\n",
    "\n",
    "# Network Parameters\n",
    "num_input = 1\n",
    "num_steps = 24\n",
    "# n_hidden = 32 # hidden layer num of features\n",
    "lstm_size = 32\n",
    "\n",
    "n_classes = 2 # total classes (yes or no)\n",
    "num_classes = 2\n",
    "\n",
    "# I have no idea. I just copied example's setting\n",
    "num_layers = 2\n",
    "\n",
    "# tf Graph input\n",
    "\n",
    "# x\n",
    "inputs = tf.placeholder(tf.int32, [None, num_steps], name='inputs')\n",
    "\n",
    "# y\n",
    "targets = tf.placeholder(tf.int32, [None, 1], name='targets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define model\n",
    "\n",
    "# Keep probability placeholder for drop out layers\n",
    "keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "# One-hot encoding the input and target characters\n",
    "x_one_hot = tf.one_hot(inputs, num_actions)\n",
    "y_one_hot = tf.one_hot(targets, num_classes)\n",
    "\n",
    "# Use a basic LSTM cell\n",
    "lstm = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "\n",
    "# Add dropout to the cell\n",
    "drop = tf.contrib.rnn.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "\n",
    "# Stack up multiple LSTM layers, for deep learning\n",
    "cell = tf.contrib.rnn.MultiRNNCell([drop] * num_layers)\n",
    "initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "### Run the data through the RNN layers\n",
    "# This makes a list where each element is on step in the sequence\n",
    "rnn_inputs = [tf.squeeze(i, squeeze_dims=[1]) for i in tf.split(x_one_hot, num_steps, 1)]\n",
    "\n",
    "# Run each sequence step through the RNN and collect the outputs\n",
    "outputs, state = tf.contrib.rnn.static_rnn(cell, rnn_inputs, initial_state=initial_state)\n",
    "final_state = state\n",
    "output_last = outputs[-1]\n",
    "\n",
    "# Reshape output so it's a bunch of rows, one output row for each step for each batch\n",
    "# seq_output = tf.concat(outputs, axis=1)\n",
    "# output = tf.reshape(seq_output, [-1, lstm_size])\n",
    "\n",
    "# Now connect the RNN putputs to a softmax layer\n",
    "with tf.variable_scope('softmax'):\n",
    "    softmax_w = tf.Variable(tf.truncated_normal((lstm_size, num_classes), stddev=0.1))\n",
    "    softmax_b = tf.Variable(tf.zeros(num_classes))\n",
    "\n",
    "# Since output is a bunch of rows of RNN cell outputs, logits will be a bunch\n",
    "# of rows of logit outputs, one for each step and batch\n",
    "# logits = tf.matmul(output, softmax_w) + softmax_b\n",
    "# we care only the last\n",
    "\n",
    "# calculate logits from output_last and weight and bias\n",
    "logits = tf.matmul(output_last, softmax_w) + softmax_b\n",
    "\n",
    "# Use softmax to get the probabilities for predicted characters\n",
    "preds = tf.nn.softmax(logits, name='predictions')\n",
    "\n",
    "# Reshape the targets to match the logits\n",
    "y_reshaped = tf.reshape(y_one_hot, [-1, num_classes])\n",
    "loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y_reshaped)\n",
    "cost = tf.reduce_mean(loss)\n",
    "\n",
    "# Optimizer for training, using gradient clipping to control exploding gradients\n",
    "# I don't know we still needs settings below...\n",
    "\n",
    "# unkonwn settings\n",
    "grad_clip = 5\n",
    "\n",
    "tvars = tf.trainable_variables()\n",
    "grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars), grad_clip)\n",
    "train_op = tf.train.AdamOptimizer(learning_rate)\n",
    "optimizer = train_op.apply_gradients(zip(grads, tvars))\n",
    "\n",
    "# Export the nodes\n",
    "# NOTE: I'm using a namedtuple here because I think they are cool\n",
    "export_nodes = ['inputs', 'targets', 'initial_state', 'final_state',\n",
    "                'keep_prob', 'cost', 'preds', 'optimizer']\n",
    "Graph = namedtuple('Graph', export_nodes)\n",
    "local_dict = locals()\n",
    "graph = Graph(*[local_dict[each] for each in export_nodes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# accuracy\n",
    "correct_pred = tf.equal(tf.argmax(preds,1), tf.argmax(tf.squeeze(y_one_hot),1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_x.shape:(12, 24)\n",
      "batch_y.shape:(12, 1)\n",
      "step:10, test loss:0.24712484487642844, valid accuracy:0.9065040684327846\n",
      "step:20, test loss:0.22174261691824843, valid accuracy:0.9085365897271691\n",
      "step:30, test loss:0.2040601518532882, valid accuracy:0.918699191837776\n",
      "step:40, test loss:0.18529470891381303, valid accuracy:0.9207317116783886\n",
      "step:50, test loss:0.17253714457464714, valid accuracy:0.9207317073170732\n",
      "step:60, test loss:0.16088125333442196, valid accuracy:0.9227642286114577\n",
      "step:70, test loss:0.15357467344942657, valid accuracy:0.9207317073170732\n",
      "step:80, test loss:0.14222889218566706, valid accuracy:0.9308943108814519\n",
      "step:90, test loss:0.13677117583807558, valid accuracy:0.9288617939483829\n",
      "step:100, test loss:0.12550366749802683, valid accuracy:0.9451219541270558\n",
      "step:110, test loss:0.12020267584836498, valid accuracy:0.9471544768752121\n",
      "step:120, test loss:0.11643759689487827, valid accuracy:0.9491869967158247\n",
      "step:130, test loss:0.10923397427531502, valid accuracy:0.9471544754214403\n",
      "step:140, test loss:0.11018957932887133, valid accuracy:0.95325203639705\n",
      "step:150, test loss:0.10260232209907069, valid accuracy:0.9451219555808277\n",
      "step:160, test loss:0.10026755121914903, valid accuracy:0.9491869967158247\n",
      "step:170, test loss:0.09569111243918693, valid accuracy:0.9573170775320472\n",
      "step:180, test loss:0.09707522411127381, valid accuracy:0.9573170760782753\n",
      "step:190, test loss:0.09226839401502124, valid accuracy:0.9593496002802034\n",
      "step:200, test loss:0.09205543922954045, valid accuracy:0.9512195180102092\n",
      "step:210, test loss:0.08989364952644488, valid accuracy:0.9552845562376627\n",
      "step:220, test loss:0.08831225245958194, valid accuracy:0.9552845576914345\n",
      "step:230, test loss:0.08591053354886147, valid accuracy:0.9654471612558132\n",
      "step:240, test loss:0.08177365500111287, valid accuracy:0.9695122009370385\n",
      "step:250, test loss:0.0843420434371122, valid accuracy:0.9654471598020414\n",
      "step:260, test loss:0.08436919182238246, valid accuracy:0.9695122009370385\n",
      "step:270, test loss:0.08011650960077532, valid accuracy:0.9674796781888823\n",
      "step:280, test loss:0.07857233172733685, valid accuracy:0.9613821172132725\n",
      "step:290, test loss:0.07890369793249799, valid accuracy:0.9735772420720357\n",
      "step:300, test loss:0.0747012191197185, valid accuracy:0.9573170775320472\n",
      "step:310, test loss:0.07631803151904024, valid accuracy:0.9674796781888823\n",
      "step:320, test loss:0.0756107456443715, valid accuracy:0.9715447178701075\n",
      "step:330, test loss:0.07457490410294364, valid accuracy:0.973577239164492\n",
      "step:340, test loss:0.07545500913005526, valid accuracy:0.9735772406182638\n",
      "step:350, test loss:0.07322589943887578, valid accuracy:0.9776422773919454\n",
      "step:360, test loss:0.07671737540390192, valid accuracy:0.9817073214344862\n",
      "step:370, test loss:0.0733978819977862, valid accuracy:0.9817073214344862\n",
      "step:380, test loss:0.0730549804614202, valid accuracy:0.977642281753261\n",
      "step:390, test loss:0.07462748711381816, valid accuracy:0.9796748001401018\n",
      "step:400, test loss:0.07731134736026433, valid accuracy:0.9837398412750988\n",
      "step:410, test loss:0.07305282211291342, valid accuracy:0.9796748001401018\n",
      "step:420, test loss:0.07615640895919569, valid accuracy:0.973577239164492\n",
      "step:430, test loss:0.07313303983916285, valid accuracy:0.9715447207776512\n",
      "step:440, test loss:0.07203944508376177, valid accuracy:0.9756097590051046\n",
      "step:450, test loss:0.07331795181477598, valid accuracy:0.9695121980294948\n",
      "step:460, test loss:0.07291932913664369, valid accuracy:0.977642281753261\n",
      "step:470, test loss:0.07546224848615263, valid accuracy:0.9735772406182638\n",
      "step:480, test loss:0.07123622979489785, valid accuracy:0.977642281753261\n",
      "step:490, test loss:0.07152481148391719, valid accuracy:0.9756097604588765\n",
      "step:500, test loss:0.07254060788838311, valid accuracy:0.9735772406182638\n",
      "step:510, test loss:0.0753880150408122, valid accuracy:0.977642281753261\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-1e2494e9e197>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_count\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m0.7\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch_idx\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/kouohhashi/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/kouohhashi/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/kouohhashi/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Users/kouohhashi/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/kouohhashi/anaconda3/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Launch the graph\n",
    "\n",
    "# Save check points\n",
    "save_file = './favorr_lstm_02.ckpt'\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    step = 1\n",
    "    # Keep training until reach max iterations\n",
    "    # while step * batch_size < training_iters:\n",
    "    # training_iters = len(train_x) // batch_size\n",
    "    # print(training_iters)\n",
    "    while step < training_iters:\n",
    "        # batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        \n",
    "        # total loss\n",
    "        loss_total = 0\n",
    "        \n",
    "        batch_count = len(train_x) // batch_size\n",
    "        for batch_idx in range(batch_count):\n",
    "            batch_x, batch_y = get_batch(train_x, train_y, batch_idx, batch_size)\n",
    "            sess.run(optimizer, feed_dict={inputs: batch_x, targets: batch_y, keep_prob:0.7})\n",
    "            \n",
    "            if batch_idx == 0 and step == 1:\n",
    "                print(\"batch_x.shape:{}\".format(batch_x.shape))\n",
    "                print(\"batch_y.shape:{}\".format(batch_y.shape))\n",
    "        \n",
    "            # calculate loss\n",
    "            loss = sess.run(cost, feed_dict={inputs: batch_x, targets: batch_y, keep_prob:1.0})\n",
    "            loss_total = loss_total + loss\n",
    "        \n",
    "        \n",
    "        if step % display_step == 0:\n",
    "            \n",
    "            test_loss = loss_total / batch_count\n",
    "            \n",
    "            # Calculate validation accuracy\n",
    "            valid_batch_count = len(valid_x) // batch_size\n",
    "            valid_acc_total = 0\n",
    "            for batch_idx in range(valid_batch_count):\n",
    "                vaild_batch_x, valid_batch_y = get_batch(valid_x, valid_y, batch_idx, batch_size)\n",
    "                acc = sess.run(accuracy, feed_dict={inputs: vaild_batch_x, targets: valid_batch_y, keep_prob:1.0})\n",
    "                valid_acc_total = valid_acc_total + acc\n",
    "            \n",
    "            valid_acc = valid_acc_total / valid_batch_count\n",
    "            print(\"step:{}, test loss:{}, valid accuracy:{}\".format(step, test_loss, valid_acc))\n",
    "            \n",
    "\n",
    "        step += 1\n",
    "    \n",
    "    #\n",
    "    # finally Save the model\n",
    "    saver.save(sess, save_file)\n",
    "    print (\"Optimization Finished!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
